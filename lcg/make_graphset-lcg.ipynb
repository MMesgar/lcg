{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def voc_count(corpus):\n",
    "    d = defaultdict(int)\n",
    "    for p in corpus:\n",
    "        for sent in p:\n",
    "            for t in sent:\n",
    "                d[t] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#load list of stop words\n",
    "with open('./snowball_stopwords.txt','rb') as sw:\n",
    "    stop_words = [line.strip() for line in sw]\n",
    "\n",
    "#load punctuations \n",
    "punctuations = string.punctuation\n",
    "\n",
    "#extra characters\n",
    "extra = []\n",
    "\n",
    "def pre_process_par(par):\n",
    "    \"\"\"\n",
    "    input: \n",
    "       a paragraph\n",
    "    output:\n",
    "       list of sentences. Each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # make the par lowecase\n",
    "    par = par.lower()\n",
    "    \n",
    "    # split sentences\n",
    "    sent_par = sent_tokenize(par)\n",
    "    \n",
    "    # tokenize and clean all sentences\n",
    "    for sent in sent_par:\n",
    "        \n",
    "        #tokenize each sentence\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # remove repetitve words in a sentenece\n",
    "        tokens = list(set(tokens))\n",
    "        \n",
    "        #remove stop words and clean texts\n",
    "        tokens = [tok for tok in tokens if \n",
    "                                          (tok not in stop_words) and \n",
    "                                          (tok not in punctuations) and \n",
    "                                          (tok not in extra)]\n",
    "        \n",
    "        # put it in the output\n",
    "        output.append(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path = './glove.840B.300d.txt'\n",
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=1)\n",
    "\n",
    "cn = 0\n",
    "word2vec = {}\n",
    "with open(w2v_path,'rb') as w2v:\n",
    "    content = w2v.read().strip()\n",
    "    for line in content.split('\\n'):\n",
    "        cn +=1\n",
    "        line = line.strip().split()\n",
    "        v = line[0]\n",
    "        \n",
    "        vector = line[1:]\n",
    "        vector = np.matrix(vector,dtype='float32')\n",
    "        \n",
    "        word2vec[v] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_connection(sent1, sent2):\n",
    "    word_connections = []\n",
    "    for w2 in sent2:\n",
    "        max_connection = (w2,None,-1)\n",
    "        try:\n",
    "            w2_vec = word2vec[w2]\n",
    "        except:\n",
    "            word2vec[w2] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "            w2_vec = word2vec[w2]\n",
    "            \n",
    "        for w1 in sent1:\n",
    "            try:\n",
    "                w1_vec = word2vec[w1]\n",
    "            except:\n",
    "                word2vec[w1] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                w1_vec = word2vec[w1]\n",
    "        \n",
    "            # compute the cosine value\n",
    "            cosine_w2_w1 =  np.abs(1 - spatial.distance.cosine(w2_vec, w1_vec))\n",
    "            \n",
    "            if cosine_w2_w1 >= max_connection[-1]:\n",
    "                max_connection = (w2,w1,cosine_w2_w1)\n",
    "\n",
    "            # append max_connection to word_connections\n",
    "            word_connections.append(max_connection)\n",
    "        \n",
    "    # pick up the word connection with maximum weight\n",
    "    output = (None,None,-1)\n",
    "    for item in word_connections:\n",
    "        if item[-1] > output[-1]:\n",
    "            output = item\n",
    "            \n",
    "    # return output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "def create_graph(list_of_sents):\n",
    "    \n",
    "    # get the number of sentences or nodes\n",
    "    n = len(list_of_sents)\n",
    "    \n",
    "    #initialize the adjacent list for the graph representation of this text\n",
    "    adj_list = [[] for i in range(n)]\n",
    "\n",
    "    #for each sentence compute its weight with each previouse sentence   \n",
    "    for i in range(1, n):            \n",
    "            sent_i = list_of_sents[i]\n",
    "            \n",
    "            for j in range(0,i):\n",
    "                sent_j = list_of_sents[j]\n",
    "                \n",
    "\n",
    "                # compute the connection between sent_i and sent_j sen_j preceds sent_i\n",
    "                edge_ij = sentence_connection(sent_j, sent_i)\n",
    "                \n",
    "                \n",
    "                # check for the threshold\n",
    "                weight = edge_ij[-1]\n",
    "                #if weight>=0.9:\n",
    "                adj_list[i].append((j,weight))\n",
    "                \n",
    "\n",
    "    # return adj_list\n",
    "    return adj_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['us', 'barak', 'president'], ['chancellor', 'angela', 'germany'], ['angela', 'france', 'met', 'barak']]\n",
      "0 -->  []\n",
      "1 -->  [(0, 0.47426620103463102)]\n",
      "2 -->  [(0, 0.99999991593103033), (1, 0.99999996671336)]\n"
     ]
    }
   ],
   "source": [
    "dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "#dump_text = \"Mohsen likes briliant people . Ali tries hard . Mohsen is a successful mohsen person . Mohsen is smart . \"\n",
    "dump_text = pre_process_par(dump_text)\n",
    "print dump_text\n",
    "\n",
    "#sentence_connection(dump_text[1],dump_text[2])\n",
    "\n",
    "adj_list = create_graph(dump_text)\n",
    "#print word_connections\n",
    "g = adj_list\n",
    "for k, v in enumerate(g):\n",
    "    print \"%d -->  %s\"%(k,v)\n",
    "#x = word2vec['angela'].shape\n",
    "#np.abs(1 - spatial.distance.cosine(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(graph_set, path, weighted=False):\n",
    "    output_content = []\n",
    "    for graph_name,  adj_list in enumerate(graph_set):\n",
    "        output_content.append('XP')\n",
    "        output_content.append('% '+str(graph_name))\n",
    "        #output_content.append('t # %d'%graph_name)\n",
    "        num_nodes = len(adj_list)\n",
    "\n",
    "        for n in range(num_nodes):\n",
    "            output_content.append('v %s a'%str(n))\n",
    "\n",
    "        for i, edges in enumerate(adj_list):\n",
    "            if i>0:\n",
    "                for j in edges:\n",
    "                    #Note: we computed edges backward, but we should \n",
    "                    # save them forward to be compatible with NAACL16\n",
    "                    source = j[0] \n",
    "                    target = i\n",
    "                    if target > source:\n",
    "                        if source>0 and target>0:\n",
    "                            if weighted:\n",
    "                                output_content.append('d %s %s %s'%(str(source),str(target),str(j[1])))\n",
    "                            else:\n",
    "                                output_content.append('d %s %s 1'%(str(source),str(target)))\n",
    "                    else:\n",
    "                        raise ValueError(\"Backward eadge?\")\n",
    "\n",
    "    with open(path,'wb') as out:\n",
    "        out.write('\\n'.join(output_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraphs in original: 5000\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('./Hansard/hansard.en.original.10K.out', 'rb','utf8') as orig:\n",
    "    orig_paragraphs = []\n",
    "    for line in orig:\n",
    "        if line != '\\n' and len(line)>0:\n",
    "            orig_paragraphs.append(line.strip())\n",
    "print \"# paragraphs in original: %d\"%len(orig_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4880 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:   45.0s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "original_pars = []\n",
    "original_pars = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(pre_process_par), orig_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 29.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 50.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 74.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 112.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed: 159.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed: 208.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed: 227.6min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "graphs = []\n",
    "graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "                 map(delayed(create_graph), original_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('original_pars_graphs_lcg.pickle','wb') as out:\n",
    "#     pickle.dump(graphs, out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(graphs,'./orig_lcg_weighted_graph_set.g', weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraphs in translated: 5000\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('./Hansard/hansard.en.translated.from.fr.10K.out','rb','utf8') as tran:\n",
    "    tran_paragraphs = []\n",
    "    for line in tran:\n",
    "        if line != '\\n' and len(line)>0:\n",
    "            tran_paragraphs.append(line.strip())\n",
    "\n",
    "print \"# paragraphs in translated: %d\"%len(tran_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:   43.3s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "translated_pars = []\n",
    "translated_pars = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(pre_process_par), tran_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 51.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 83.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 118.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed: 159.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed: 211.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed: 230.3min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "translated_graphs = []\n",
    "translated_graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(create_graph), translated_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(translated_graphs,'./trans_lcg_weighted_graph_set.g', weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('trans_pars_graphs_lcg.pickle','wb') as out:\n",
    "#     pickle.dump(translated_graphs, out)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data first. We have two corpora: \n",
    "         - original\n",
    "         - translated\n",
    "Each dataset consists of some paragraphs. Paragraphs are seperated by an empty line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of paragraphs. We need to pre-process each paragraph and remove all stop words from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should process and clean all pragraphs in both corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a paragraph and build its graph. In this regard we define a function that takes a list of sentenecs (tokenized) as an input text and returns a graph (in ?????? format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['people', 'mohsen', 'likes', 'ali'], ['tries', 'hard', 'mohsen', 'ali'], ['successful', 'mohsen', 'person'], ['mohsen', 'smart']]\n",
      "0 -->  []\n",
      "1 -->  [(0, 1.0000000580073984)]\n",
      "2 -->  [(0, 1.0000000580073984), (1, 1.0000000580073984)]\n",
      "3 -->  [(0, 1.0000000580073984), (1, 1.0000000580073984), (2, 1.0000000580073984)]\n"
     ]
    }
   ],
   "source": [
    "dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "dump_text = \"Mohsen likes Ali people . Mohsen tries Ali hard . Mohsen is a successful person . Mohsen is smart . \"\n",
    "dump_text = pre_process_par(dump_text)\n",
    "print dump_text\n",
    "adj_list = create_graph(dump_text)\n",
    "\n",
    "g = adj_list\n",
    "for k, v in enumerate(g):\n",
    "    print \"%d -->  %s\"%(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_connections, adj_list = graphs[0]\n",
    "#for k,v in enumerate(adj_list):\n",
    "#    if len(v)>1:\n",
    "#        print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = graphs[0][1]\n",
    "#for k, v in enumerate(g):\n",
    "#    print \"%d --> %d %s\"%(k,v[0],v[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct graphs for translationese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
