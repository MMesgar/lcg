{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def voc_count(corpus):\n",
    "    d = defaultdict(int)\n",
    "    for p in corpus:\n",
    "        for sent in p:\n",
    "            for t in sent:\n",
    "                d[t] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#load list of stop words\n",
    "with open('./snowball_stopwords.txt','rb') as sw:\n",
    "    stop_words = [line.strip() for line in sw]\n",
    "\n",
    "#load punctuations \n",
    "punctuations = string.punctuation\n",
    "\n",
    "#extra characters\n",
    "extra = []\n",
    "\n",
    "def pre_process_par(par):\n",
    "    \"\"\"\n",
    "    input: \n",
    "       a paragraph\n",
    "    output:\n",
    "       list of sentences. Each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # make the par lowecase\n",
    "    par = par.lower()\n",
    "    \n",
    "    # split sentences\n",
    "    sent_par = sent_tokenize(par)\n",
    "    \n",
    "    # tokenize and clean all sentences\n",
    "    for sent in sent_par:\n",
    "        \n",
    "        #tokenize each sentence\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # remove repetitve words in a sentenece\n",
    "        tokens = list(set(tokens))\n",
    "        \n",
    "        #remove stop words and clean texts\n",
    "        tokens = [tok for tok in tokens if \n",
    "                                          (tok not in stop_words) and \n",
    "                                          (tok not in punctuations) and \n",
    "                                          (tok not in extra)]\n",
    "        \n",
    "        # put it in the output\n",
    "        output.append(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path = './glove.840B.300d.txt'\n",
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=1)\n",
    "\n",
    "cn = 0\n",
    "word2vec = {}\n",
    "with open(w2v_path,'rb') as w2v:\n",
    "    content = w2v.read().strip()\n",
    "    for line in content.split('\\n'):\n",
    "        cn +=1\n",
    "        line = line.strip().split()\n",
    "        v = line[0]\n",
    "        \n",
    "        vector = line[1:]\n",
    "        vector = np.matrix(vector,dtype='float32')\n",
    "        \n",
    "        word2vec[v] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_connection(sent1, sent2):\n",
    "    word_connections = []\n",
    "    for w2 in sent2:\n",
    "        max_connection = (w2,None,-1)\n",
    "        try:\n",
    "            w2_vec = word2vec[w2]\n",
    "        except:\n",
    "            word2vec[w2] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "            w2_vec = word2vec[w2]\n",
    "            \n",
    "        for w1 in sent1:\n",
    "            try:\n",
    "                w1_vec = word2vec[w1]\n",
    "            except:\n",
    "                word2vec[w1] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                w1_vec = word2vec[w1]\n",
    "        \n",
    "            # compute the cosine value\n",
    "            cosine_w2_w1 =  np.abs(1 - spatial.distance.cosine(w2_vec, w1_vec))\n",
    "            \n",
    "            if cosine_w2_w1 >= max_connection[-1]:\n",
    "                max_connection = (w2,w1,cosine_w2_w1)\n",
    "\n",
    "            # append max_connection to word_connections\n",
    "            word_connections.append(max_connection)\n",
    "        \n",
    "    # pick up the word connection with maximum weight\n",
    "    output = (None,None,-1)\n",
    "    for item in word_connections:\n",
    "        if item[-1] > output[-1]:\n",
    "            output = item\n",
    "            \n",
    "    # return output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "def create_graph(list_of_sents):\n",
    "    \n",
    "    # get the number of sentences or nodes\n",
    "    n = len(list_of_sents)\n",
    "    \n",
    "    #initialize the adjacent list for the graph representation of this text\n",
    "    adj_list = [[] for i in range(n)]\n",
    "\n",
    "    #for each sentence compute its weight with each previouse sentence   \n",
    "    for i in range(1, n):            \n",
    "            sent_i = list_of_sents[i]\n",
    "            \n",
    "            for j in range(0,i):\n",
    "                sent_j = list_of_sents[j]\n",
    "                \n",
    "\n",
    "                # compute the connection between sent_i and sent_j sen_j preceds sent_i\n",
    "                edge_ij = sentence_connection(sent_j, sent_i)\n",
    "                \n",
    "                \n",
    "                # check for the threshold\n",
    "                weight = edge_ij[-1]\n",
    "                #if weight>=0.9:\n",
    "                adj_list[i].append((j,weight))\n",
    "                \n",
    "\n",
    "    # return adj_list\n",
    "    return adj_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['us', 'barak', 'president'], ['chancellor', 'angela', 'germany'], ['angela', 'france', 'met', 'barak']]\n",
      "0 -->  []\n",
      "1 -->  [(0, 0.47426614165306091)]\n",
      "2 -->  [(0, 1.0), (1, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "#dump_text = \"Mohsen likes briliant people . Ali tries hard . Mohsen is a successful mohsen person . Mohsen is smart . \"\n",
    "dump_text = pre_process_par(dump_text)\n",
    "print dump_text\n",
    "\n",
    "#sentence_connection(dump_text[1],dump_text[2])\n",
    "\n",
    "adj_list = create_graph(dump_text)\n",
    "#print word_connections\n",
    "g = adj_list\n",
    "for k, v in enumerate(g):\n",
    "    print \"%d -->  %s\"%(k,v)\n",
    "#x = word2vec['angela'].shape\n",
    "#np.abs(1 - spatial.distance.cosine(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(graph_set, path, weighted=False):\n",
    "    output_content = []\n",
    "    for graph_name,  adj_list in enumerate(graph_set):\n",
    "        output_content.append('XP')\n",
    "        output_content.append('% '+str(graph_name))\n",
    "        #output_content.append('t # %d'%graph_name)\n",
    "        num_nodes = len(adj_list)\n",
    "\n",
    "        for n in range(num_nodes):\n",
    "            output_content.append('v %s a'%str(n))\n",
    "\n",
    "        for i, edges in enumerate(adj_list):\n",
    "            if i>0:\n",
    "                for j in edges:\n",
    "                    #Note: we computed edges backward, but we should \n",
    "                    # save them forward to be compatible with NAACL16\n",
    "                    source = j[0] \n",
    "                    target = i\n",
    "                    if target > source:\n",
    "                        if source>0 and target>0:\n",
    "                            if weighted:\n",
    "                                output_content.append('d %s %s %s'%(str(source),str(target),str(j[1])))\n",
    "                            else:\n",
    "                                output_content.append('d %s %s 1'%(str(source),str(target)))\n",
    "                    else:\n",
    "                        raise ValueError(\"Backward eadge?\")\n",
    "\n",
    "    with open(path,'wb') as out:\n",
    "        out.write('\\n'.join(output_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraphs in original: 3\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('./texts', 'rb','utf8') as orig:\n",
    "    orig_paragraphs = []\n",
    "    for line in orig:\n",
    "        if line != '\\n' and len(line)>0:\n",
    "            orig_paragraphs.append(line.strip())\n",
    "print \"# paragraphs in original: %d\"%len(orig_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "original_pars = []\n",
    "original_pars = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(pre_process_par), orig_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   32.2s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "graphs = []\n",
    "graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "                 map(delayed(create_graph), original_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('original_pars_graphs_lcg.pickle','wb') as out:\n",
    "#     pickle.dump(graphs, out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(graphs,'./graph_set.g', weighted=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
